from openai import OpenAI
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer
from datasets import load_dataset
from peft import AutoPeftModelForCausalLM
from huggingface_hub import login
import torch
from random import sample
from tqdm import tqdm
from random import sample
import os
import gc
import json
import argparse
import re
import time


def init_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument('--generation_model', type=str, help="Model to use: [llama-sft|llama-instruct-sft|llama-instruct|qwen-sft|qwen-instruct-sft|qwen-instruct]", default='qwen-sft')
    parser.add_argument('--refinement_model', type=str, help="Model to use: [llama-sft|llama-instruct-sft|llama-instruct|qwen-sft|qwen-instruct-sft|qwen-instruct]", default='gpt-4o-mini')
    parser.add_argument('--setting', type=str, default='7b')
    parser.add_argument('--aspect', type=str, default="")
    parser.add_argument('--few_shot', action='store_true')
    parser.add_argument('--range1', type=int, default=0)
    parser.add_argument('--range2', type=int)
    parser.add_argument('--case_study', action='store_true')
    parser.add_argument('--no-case_study', dest='case_study', action='store_false')
    parser.add_argument('--no-few_shot', action='store_false', dest="few_shot")
    parser.add_argument('--few_shot_once', action='store_true')
    parser.add_argument('--no-few_shot_once', action='store_false', dest="few_shot_once")
    parser.add_argument('--few_shot_refine', action='store_true')
    parser.add_argument('--no-few_shot_refine', action='store_false', dest="few_shot_refine")
    parser.add_argument('--include_past_history', action='store_true')
    parser.add_argument('--max_gen_token', type=int, default=512)
    parser.add_argument('--max_refine_token', type=int, default=512)
    parser.add_argument('--no-include_past_history', dest='include_past_history', action='store_false')
    parser.add_argument('--dataset', type=str, help="sample or full dataset: [sample|validation|test]", default='sample')
    parser.add_argument('--output', type=str)
    parser.add_argument('--iter', type=int, default=3)
    parser.add_argument('--use_refine_model', action='store_true')
    parser.add_argument('--no_use_refine_model', dest='use_refine_model', action='store_false')

    parser.set_defaults(use_refine_model=False)
    parser.set_defaults(few_shot=False)
    parser.set_defaults(few_shot_once=False)
    parser.set_defaults(few_shot_refine=False)
    parser.set_defaults(include_past_history=False)
    parser.set_defaults(case_study=False)
    return parser

def extract_total_score(feedback: str) -> float:
    # Match numbers followed by optional spaces and then '/10'
    fb = feedback.split('Total Score')[0].strip()
    matches = re.findall(r'(\d+(?:\.\d+)?)\s*/\s*10', fb)

    if len(matches) == 0:
        print(feedback)
        return 'Error'

    # Sum the first 4 scores only
    
    scores = [float(score) for score in matches[:3]]
    
    return sum(scores)



def generation(item, example):
    prompt = f"""### You are translating professional radiology reports into layman's terms. Do not include any medical jargon. Write concisely. When rewriting the radiology reports, follow these examples:
    Radiology Report:\n{example[0]['radiology_report']}\n\nLayman's Report: {example[0]['layman_report']}\n
    Radiology Report:\n{example[1]['radiology_report']}\n\nLayman's Report: {example[1]['layman_report']}\n
    Radiology Report:\n{example[2]['radiology_report']}\n\nLayman's Report: {example[2]['layman_report']}"""

    query = f"### Radiology Report:\n{item} ### Layman's Report:"

    
    return [{"role": "system", "content": prompt},
            {"role": "user", "content": query}]

def one_feedback_prompt(item, lay_report, examples, aspect, refined_prompt=""):
    prompt = f"""### You are an expert medical language reviewer. You are given a radiology report and the output generated by a language model in response to it. Evaluate the {aspect} of the **entire model output** (not just the lay report section) on a scale of one to ten.
    Provide a concise explanation and a score in the format x/10."""

    if aspect.lower() == 'factuality':
        prompt += "\n**Factuality (x/10)**: How factually consistent is the output with the original radiology report? Highlight factually incorrect or inconsistent phrases and penalize accordingly."
    elif aspect.lower() == 'readability':
        prompt += "\n**Readability (x/10)**: Is the output easy to understand for a patient with no background in medicine? Identify medical terms or unclear phrasing and penalize as needed."
    elif aspect.lower() == 'completeness':
        prompt += "\n**Completeness (x/10)**: Does the output include all important information from the radiology report? Penalize omissions."
    elif aspect.lower() == 'conciseness':
        prompt += "\n**Conciseness (x/10)**: Is the output concise and succinct? Penalize unnecessarily verbose outputs (e.g., Outputs that over-explain, or repetitive outputs)."
    elif aspect.lower() == 'writing style':
        prompt += "\n**Writing Style (x/10)**: Is the tone formal, objective, and clinical? Penalize conversational phrasing, direct address (e.g., “you”), or quoting of the original report."
    elif aspect.lower() == 'structure':
        prompt += "\n**Structure (x/10)**: Does the output follow a clear paragraph-based structure similar to clinical reports? Penalize if it uses headings, bullet points, or numbered lists."
    elif aspect.lower() == 'format':
        prompt += "\n**Format (x/10)**: Penalize any commentary or non-report language, such as “Here is your revised report,” “Translation:”, or any explanation of changes. Full marks only if the output **only** contains the lay summary, without extra headers or commentary."

    if not refined_prompt:
        pass

    elif refined_prompt and isinstance(refined_prompt, str):
        if args.include_past_history:
            if 'past edits' not in prompt:
                prompt += "Here are past edits for your reference:\n"
            prompt += f"{refined}\n\n"

    query = f"""## Original Radiology Report:
    {item}

    ## Lay Report:
    {lay_report}

    ## Feedback:"""

    return [{"role": "system", "content": prompt},
            {"role": "user", "content": query}]


def feedback_prompt(item, lay_report, examples, refined_prompt=""):
    prompt = f"""### You are an expert medical language reviewer. You are given a radiology report and the full output generated by a language model in response to it. Evaluate the quality of the **entire model output** (not just the lay report section) based on the following 3 criteria.

    For each, provide a **concise explanation (1-2 sentences max)** and a **score in the format x/10**. At the end, provide the total score as the **sum of all three criteria**, formatted as **n/30**.
    1. **Factuality (x/10)**: How factually consistent is the output with the original radiology report? Highlight factually incorrect or inconsistent phrases and penalize accordingly.
    2. **Completeness (x/10)**: Does the output include all important information from the radiology report? Penalize omissions.
    3. **Format (x/10)**: Penalize any commentary or non-report language, such as “Here is your revised report,” “Translation:”, or any explanation of changes. Full marks only if the output **only** contains the lay summary, without extra headers or commentary.
    4. **Total Score (n/30)**: Sum of the seven individual scores.
    """

    if args.few_shot:
            prompt += f"""Here are some examples of evaluations:
        Original Radiology Report:
        {examples[0]['radiology_report']}

        Lay Report:
        {examples[0]['lay_report']}

        Feedback:
        {examples[0]['feedback']}

        
        Original Radiology Report:
        {examples[1]['radiology_report']}

        Lay Report:
        {examples[1]['lay_report']}

        Feedback:
        {examples[1]['feedback']}


        Original Radiology Report:
        {examples[2]['radiology_report']}

        Lay Report:
        {examples[2]['lay_report']}

        Feedback:
        {examples[2]['feedback']}\n"""
    if not refined_prompt:
        if args.few_shot_once:
            prompt += f"""Here are some examples of evaluations.
        Original Radiology Report:
        {examples[0]['radiology_report']}

        Lay Report:
        {examples[0]['lay_report']}

        Feedback:
        {examples[0]['feedback']}

        
        Original Radiology Report:
        {examples[1]['radiology_report']}

        Lay Report:
        {examples[1]['lay_report']}

        Feedback:
        {examples[1]['feedback']}


        Original Radiology Report:
        {examples[2]['radiology_report']}

        Lay Report:
        {examples[2]['lay_report']}

        Feedback:
        {examples[2]['feedback']}\n"""

    elif refined_prompt and isinstance(refined_prompt, str):
        if args.include_past_history:
            if 'past edits' not in prompt:
                prompt += "Here are past edits for your reference:\n"
            prompt += f"{refined}\n\n"

    query = f"""## Original Radiology Report:
    {item}

    ## Lay Report:
    {lay_report}

    ## Feedback:"""

    return [{"role": "system", "content": prompt},
            {"role": "user", "content": query}]

def refinement(feedback, item, lay_report, feedback_query=""):
    prompt = """### You are translating radiology reports into layman's terms. You are given feedback for a lay report. Use the given feedback to improve and rewrite the lay report.
    Do not include any commentary, section titles, or explanation of any changes made. The output should contain only the lay report, written clearly."""

    if args.few_shot_refine:
        example = sample(examples, 3)
        prompt += f"""Here are examples of radiology report translation:
        
        Radiology Report:
        {example[0]['radiology_report']}

        Layman Report:
        {example[0]['layman_report']}

        Radiology Report:
        {example[1]['layman_report']}

        Layman Report:
        {example[1]['layman_report']}

        Radiology Report:
        {example[2]['layman_report']}

        Layman_Report:
        {example[2]['layman_report']}"""

    if args.include_past_history:
        if feedback_query:
            if 'past feedbacks' not in prompt:
                prompt += "Here are past feedbacks for your reference:\n"
            prompt += f"{feedback_query}\n\n"

    query = f"""### Original Radiology Report:
    {item}
    
    ### Model Output:
    {lay_report}

    ### Feedback:
    {feedback}
    
    ### Use the feedback to improve the lay report. 
    ### Revised Lay Report:"""

    return [{"role": "system", "content": prompt},
            {"role": "user", "content": query}]

gc.collect()
torch.cuda.empty_cache()

parser = init_parser()
args = parser.parse_args()


login(os.environ['HUGGINGFACE_TOKEN'])
if 'gpt' in args.generation_model or 'gpt' in args.refinement_model:
    client = OpenAI(api_key=os.environ['OPENAI_KEY'])

if args.case_study:
    dataset = load_dataset("BioLaySumm/BioLaySumm2025-LaymanRRG-opensource-track", split='validation')
    data = []
    for item in dataset:
        data.append({
            'radiology_report': item['radiology_report'],
            'layman_report': item['layman_report']
        })

    del dataset
    dataset = sample(data, 5)
else:
    if 'sample' in args.dataset:
        with open(f'{args.dataset}.json', 'r') as f:
            dataset = json.load(f)
        
    else:
        dataset = load_dataset("BioLaySumm/BioLaySumm2025-LaymanRRG-opensource-track", split=args.dataset)

with open('examples_3shot.json', 'r') as f:
    examples = json.load(fp=f)

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_compute_dtype=torch.bfloat16
)


gen_model = args.generation_model
setting = args.setting

if 'gpt' not in gen_model:
    if 'qwen3' in gen_model or '3b' in setting or '4b' in setting:
        model = AutoModelForCausalLM.from_pretrained(f'../models/{gen_model}/merged_final', device_map='auto', trust_remote_code=True, low_cpu_mem_usage=True, local_files_only=True, attn_implementation='flash_attention_2')
        tokenizer = AutoTokenizer.from_pretrained(f'./models/{gen_model}/merged_final', trust_remote_code=True)
    elif '7b' in setting or '8b' in setting:
        model = AutoModelForCausalLM.from_pretrained(f'../models_7B/{gen_model}/merged_final', device_map='auto', trust_remote_code=True, low_cpu_mem_usage=True, local_files_only=True, attn_implementation="flash_attention_2")
        tokenizer = AutoTokenizer.from_pretrained(f'../models_7B/{gen_model}/merged_final', trust_remote_code=True)


if 'gpt' in args.refinement_model:
    if 'gpt' not in args.refinement_model:
        refine_model = f'gpt-{args.refinement_model}'
    else:
        refine_model = args.refinement_model
else:
    if 'SFT' in args.refinement_model:
        if 'qwen3' in args.refinement_model or '3b' in setting or '4b' in setting:
            refine_model = AutoModelForCausalLM.from_pretrained(f'../models/{args.refinement_model}/merged_final', device_map='auto', trust_remote_code=True, low_cpu_mem_usage=True, local_files_only=True, attn_implementation='flash_attention_2')
            refine_tokenizer = AutoTokenizer.from_pretrained(f'./models/{args.refinement_model}/merged_final', trust_remote_code=True)
        elif '7b' in setting or '8b' in setting:
            refine_model = AutoModelForCausalLM.from_pretrained(f'../models_7B/{args.refinement_model}/merged_final', device_map='auto', trust_remote_code=True, low_cpu_mem_usage=True, local_files_only=True, attn_implementation='flash_attention_2')
            refine_tokenizer = AutoTokenizer.from_pretrained(f'../models_7B/{args.refinement_model}/merged_final', trust_remote_code=True)
    else:
        if args.refinement_model == 'qwen-instruct':
            if '3b' in setting:
                refine_model = AutoModelForCausalLM.from_pretrained(
                    'Qwen/Qwen2.5-3B-Instruct',
                    quantization_config=quantization_config,
                    device_map='auto',
                    attn_implementation="flash_attention_2"
                )
                refine_tokenizer = AutoTokenizer.from_pretrained(f'Qwen/Qwen2.5-3B-Instruct', device_map='auto')
            elif '7b' in setting:
                refine_model = AutoModelForCausalLM.from_pretrained(
                    'Qwen/Qwen2.5-7B-Instruct',
                    quantization_config=quantization_config,
                    device_map='auto',
                    attn_implementation="flash_attention_2"
                )
                refine_tokenizer = AutoTokenizer.from_pretrained(f'Qwen/Qwen2.5-7B-Instruct', device_map='auto')
        elif args.refinement_model == 'qwen3':
            if '4b' in setting:    
                refine_model = AutoModelForCausalLM.from_pretrained(
                    'Qwen/Qwen3-4B',
                    quantization_config=quantization_config,
                    device_map='auto',
                    attn_implementation="flash_attention_2"
                )
                refine_tokenizer = AutoTokenizer.from_pretrained(f'Qwen/Qwen3-4B', device_map='auto')
            elif '8b' in setting:
                refine_model = AutoModelForCausalLM.from_pretrained(
                    'Qwen/Qwen3-8B',
                    quantization_config=quantization_config,
                    device_map='auto',
                    attn_implementation="flash_attention_2"
                )
                refine_tokenizer = AutoTokenizer.from_pretrained(f'Qwen/Qwen3-8B', device_map='auto')
first_gen = []
feedbacks_list = []
refined = []

with open('feedbacks.json', 'r') as file:
    feedback_examples = json.load(file)

if not args.range1 or not args.range2:
    range1 = 0
    range2 = len(dataset)
else:
    range1 = args.range1
    range2 = args.range2

for i in tqdm(range(range1, range2), desc="Generating"):
    if args.dataset == 'sample1':
        input_text = "Pulmonary infiltrate in the right upper lobe with a faint central opacity in the right lower lung field and peripheral opacity at the transition between the left upper and middle lung fields. Given the described radiological findings and the current epidemiological context, it is recommended to rule out COVID-19 infection. Mild to moderate anterior wedging of T6 vertebral body, already present in a previous study dated [date]."
        gold = "There is a shadow in the upper part of the right lung, and a faint central shadow in the lower part of the right lung, along with a peripheral shadow at the transition between the upper and middle parts of the left lung. Considering the current situation and these findings, it is suggested to check for COVID-19 infection. There is a mild to moderate change in the front part of the T6 vertebrae, which was also seen in a previous study from [date]."
        document = "The report shows an abnormal shadowing in the upper part of the right lung, along with a faint cloudy spot in the lower right lung and another cloudiness near the boundary of the left upper and middle lungs. Considering what we know about how diseases spread now, it\u2019s important to check if this could be caused by COVID-19. Additionally, there is a mild to moderate change in one of the bones in the upper back called T6, which was also seen in a previous test from [date]."
    
    else:
        ex_3shot = sample(examples, 3)
        input_text = dataset[i]['radiology_report']
        gold = dataset[i]['layman_report']

        if 'gpt' not in gen_model:
            if 'qwen3' in gen_model:
                text = tokenizer.apply_chat_template(
                    generation(input_text, ex_3shot),
                    tokenize=False,
                    add_generation_prompt=True,
                    enable_thinking=False
                )
            else:
                text = tokenizer.apply_chat_template(
                    generation(input_text, ex_3shot),
                    tokenize=False,
                    add_generation_prompt=True
                )

            model_inputs = tokenizer([text], return_tensors='pt').to('cuda')
            with torch.inference_mode():
                output = model.generate(
                    **model_inputs,
                    max_new_tokens=args.max_gen_token,
                    top_k=50,
                    top_p=0.9,
                    do_sample=True,
                    eos_token_id=tokenizer.eos_token_id,
                    pad_token_id=tokenizer.eos_token_id,
                    repetition_penalty=1.2
                )

            del model_inputs
            document = tokenizer.batch_decode(output, skip_special_tokens=True)
            del output
            if 'qwen3' not in gen_model:
                document = document[0].split('assistant')[1].strip()
            else:
                document = document[0].split('</think>')[1].strip()
        
        else:
            completion = client.chat.completions.create(
            model=gen_model,
            messages=generation(input_text, ex_3shot),
            temperature=0.3
        )
            
            document = completion.choices[0].message.content
            del completion

        first_gen.append({
            'document': input_text,
            'reference': gold,
            'generated_caption': document
        })

    iter_count = 0    
    feedbacks = ""
    refine = ""
    old_feedback = ""
    old_refine = ""
    old_iter = ""
    old_score = 0


    while iter_count < args.iter:
        example = sample(feedback_examples, 3)

        if not feedbacks:
            if not args.aspect:
                feedbacks = feedback_prompt(input_text, document, example)
            else:
                feedbacks = one_feedback_prompt(input_text, document, example, args.aspect)

        if 'gpt' in args.refinement_model:
            completion = client.chat.completions.create(
                model=refine_model,
                messages=feedbacks,
                temperature=0.3
            )

            feedback = completion.choices[0].message.content
            del completion
        else:
            if 'qwen3' in args.refinement_model:
                text = refine_tokenizer.apply_chat_template(
                    feedbacks,
                    tokenize=False,
                    add_generation_prompt=True,
                    enable_thinking=False
                )
            else:
                text = refine_tokenizer.apply_chat_template(
                    feedbacks,
                    tokenize=False,
                    add_generation_prompt=True
                )

            model_inputs = refine_tokenizer([text], return_tensors='pt').to('cuda')
        
            output = refine_model.generate(
                **model_inputs,
                max_new_tokens=1024,
                top_k=50,
                top_p=0.9,
                do_sample=True,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.2
            )
            del model_inputs
            feedback = refine_tokenizer.batch_decode(output, skip_special_tokens=True)
            del output
            if 'qwen3' not in args.refinement_model:
                feedback = feedback[0].split("\nassistant\n")[1].strip()
            else:
                feedback = feedback[0].split('\n</think>\n')[1].strip()
        score = extract_total_score(feedback)
        if args.case_study:
            feedbacks_list.append({
                'radiology_report': input_text,
                'document': document,
                'feedback': feedback,
                'score': score
            })
        else:
            if isinstance(score, str):
                iter_count -= 1
                score = 0

            if score >= 27:
                break
        

        if not refine:
            refine = refinement(feedback, input_text, document)

        if ('gpt' in args.refinement_model and args.use_refine_model) or 'gpt' in gen_model:
            completion = client.chat.completions.create(
                model=refine_model,
                messages=refine,
                temperature=0.3,
            )

            document = completion.choices[0].message.content
            del completion

        else:
            if 'qwen3' in gen_model:
                text = tokenizer.apply_chat_template(
                    refine,
                    tokenize=False,
                    add_generation_prompt=True,
                    enable_thinking=False
                )
            else:
                text = tokenizer.apply_chat_template(
                    refine,
                    tokenize=False,
                    add_generation_prompt=True
                )
            model_inputs = tokenizer([text], return_tensors='pt').to('cuda')
        
            output = model.generate(
                **model_inputs,
                max_new_tokens=args.max_refine_token,
                top_k=50,
                top_p=0.90,
                do_sample=True,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.2
            )

            del model_inputs
            document = tokenizer.batch_decode(output, skip_special_tokens=True)
            del output
            if 'qwen3' not in gen_model:
                document = document[0].split('assistant')[1].strip()
            else:
                document = document[0].split('</think>')[1].strip()
        iter_count += 1

        if args.include_past_history:
            old_refine += refine[1]['content'] + document
            old_feedback += feedbacks[1]['content'] + feedback
        else:
            old_refine = 1
            old_feedback = 1

        if not args.aspect:
            feedbacks = feedback_prompt(input_text, document, example, old_refine)
        else:
            feedbacks = one_feedback_prompt(input_text, document, example, args.aspect, old_refine)
        refine = refinement(feedback, input_text, document, old_feedback)

        iter_count += 1


    if not args.aspect:
        refined.append({
            'doc_id': i,
            'document': input_text,
            'reference': gold,
            'generated_caption': document
        })
    else:
        refined.append({
            'aspect': args.aspect,
            'document': input_text,
            'reference': gold,
            'generated_caption': document
        })

    del input_text
    del gold
    del document
    
if args.case_study:

    os.makedirs('../case_study', exist_ok=True)
    with open('../case_study/first_gen.json', 'w') as file:
        json.dump(first_gen, fp=file, indent=2)

    with open('../case_study/feedback.json', 'w') as file:
        json.dump(feedbacks_list, fp=file, indent=2)

if 'test' in args.dataset:
    os.makedirs('../results_test', exist_ok=True)
    dir = os.listdir('../results_test')
    if args.output not in dir:
        with open(f'../results_test/{args.output}', 'w') as file:
            json.dump(refined, fp=file, indent=2)
    else:
        with open(f'../results_test/{args.output}', 'r') as file:
            refined_results = json.load(file)
        refined_results.extend(refined)

        with open(f'../results_test/{args.output}', 'w') as file:
            json.dump(refined_results, fp=file, indent=2)



elif not args.aspect:
    os.makedirs('../refine_results', exist_ok=True)
    with open(f"../refine_results/{args.output}", 'w') as f:
        json.dump(refined, fp=f, indent=2)
else:
    os.makedirs('../refine_results_aspects', exist_ok=True)
    with open(f"../refine_results/{args.output}", 'w') as f:
        json.dump(refined, fp=f, indent=2)

